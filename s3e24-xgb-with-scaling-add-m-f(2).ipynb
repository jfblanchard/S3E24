{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# S3E24 Notebook\n",
    "* version this: add the gender prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:39:34.243771875Z",
     "start_time": "2023-11-07T21:39:32.576414749Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q scienceplots\n",
    "!pip install -q catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T21:39:36.422433575Z",
     "start_time": "2023-11-07T21:39:35.542893404Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T21:39:36.439093399Z",
     "start_time": "2023-11-07T21:39:36.398593014Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:39:37.073359171Z",
     "start_time": "2023-11-07T21:39:37.056161800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, pathlib\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "#import plotly.express as px\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use(['science','no-latex'])  # not sure how to set up latex in kaggle yet.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:40:45.181303288Z",
     "start_time": "2023-11-07T21:40:45.051415181Z"
    }
   },
   "outputs": [],
   "source": [
    "psg = True\n",
    "\n",
    "if psg:\n",
    "    root_dir = '.'\n",
    "else:\n",
    "    root_dir = '/kaggle/input/playground-series-s3e24'\n",
    "    \n",
    "train = pd.read_csv(root_dir + '/train.csv')\n",
    "test = pd.read_csv(root_dir + '/test.csv')\n",
    "ss = pd.read_csv(root_dir + '/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T21:40:46.600167199Z",
     "start_time": "2023-11-07T21:40:46.593536601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:40:54.034443655Z",
     "start_time": "2023-11-07T21:40:54.004101638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['age',\n 'height(cm)',\n 'weight(kg)',\n 'waist(cm)',\n 'eyesight(left)',\n 'eyesight(right)',\n 'hearing(left)',\n 'hearing(right)',\n 'systolic',\n 'relaxation',\n 'fasting blood sugar',\n 'Cholesterol',\n 'triglyceride',\n 'HDL',\n 'LDL',\n 'hemoglobin',\n 'Urine protein',\n 'serum creatinine',\n 'AST',\n 'ALT',\n 'Gtp',\n 'dental caries']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = [col for col in test.columns if col !='id']\n",
    "int_features = [col for col in test.columns if 'id' not in col and test[col].dtype=='int']\n",
    "float_features = [col for col in test.columns if test[col].dtype=='float']\n",
    "\n",
    "features = all_features\n",
    "features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add M/F predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:02.353784518Z",
     "start_time": "2023-11-07T21:41:02.259042886Z"
    }
   },
   "outputs": [],
   "source": [
    "#model_gndr = joblib.load('/kaggle/input/s3e24-gender-prediction-model/rf_gender_prediction.joblib')\n",
    "model_gndr = joblib.load(root_dir + '/xgb_gndr_model.joblib')\n",
    "\n",
    "train_gndr = model_gndr.predict_proba(train[features])[:,1]\n",
    "test_gndr = model_gndr.predict_proba(test[features])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:04.150720229Z",
     "start_time": "2023-11-07T21:41:04.146811450Z"
    }
   },
   "outputs": [],
   "source": [
    "train['gender'] = train_gndr.round().astype('int')\n",
    "test['gender'] = test_gndr.round().astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:08.576760130Z",
     "start_time": "2023-11-07T21:41:08.569276859Z"
    }
   },
   "outputs": [],
   "source": [
    "# add to features arrays\n",
    "features.append('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:09.707533222Z",
     "start_time": "2023-11-07T21:41:09.665677494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  eyesight(right)  \\\n0   55         165          60       81.0             0.5              0.6   \n1   70         165          65       89.0             0.6              0.7   \n2   20         170          75       81.0             0.4              0.5   \n3   35         180          95      105.0             1.5              1.2   \n4   30         165          60       80.5             1.5              1.0   \n5   50         170          55       51.0             1.2              1.2   \n6   45         160          55       69.0             1.5              1.2   \n7   55         155          60       84.5             0.7              0.9   \n8   40         165          70       89.0             0.7              1.0   \n9   40         155          50       73.0             1.5              1.5   \n\n   hearing(left)  hearing(right)  systolic  relaxation  fasting blood sugar  \\\n0              1               1       135          87                   94   \n1              2               2       146          83                  147   \n2              1               1       118          75                   79   \n3              1               1       131          88                   91   \n4              1               1       121          76                   91   \n5              1               1       146          95                  101   \n6              1               1       150          88                   84   \n7              1               1       137          91                  100   \n8              1               1       130          80                  104   \n9              1               1       105          70                   64   \n\n   Cholesterol  triglyceride  HDL  LDL  hemoglobin  Urine protein  \\\n0          172           300   40   75        16.5              1   \n1          194            55   57  126        16.2              1   \n2          178           197   45   93        17.4              1   \n3          180           203   38  102        15.9              1   \n4          155            87   44   93        15.4              1   \n5          199           343   31   99        15.9              1   \n6          222           153   69  122        13.0              1   \n7          282           165   51  198        14.5              1   \n8          243           163   59  150        15.7              1   \n9          183            27   55  122        13.2              1   \n\n   serum creatinine  AST  ALT  Gtp  dental caries  gender  \n0               1.0   22   25   27              0       1  \n1               1.1   27   23   37              1       1  \n2               0.8   27   31   53              0       1  \n3               1.0   20   27   30              1       1  \n4               0.8   19   13   17              0       1  \n5               0.7   24   42  119              1       1  \n6               0.7   17   12   16              0       0  \n7               0.7   16   15   16              0       0  \n8               0.9   24   21   31              0       1  \n9               0.7   22   16   14              0       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>height(cm)</th>\n      <th>weight(kg)</th>\n      <th>waist(cm)</th>\n      <th>eyesight(left)</th>\n      <th>eyesight(right)</th>\n      <th>hearing(left)</th>\n      <th>hearing(right)</th>\n      <th>systolic</th>\n      <th>relaxation</th>\n      <th>fasting blood sugar</th>\n      <th>Cholesterol</th>\n      <th>triglyceride</th>\n      <th>HDL</th>\n      <th>LDL</th>\n      <th>hemoglobin</th>\n      <th>Urine protein</th>\n      <th>serum creatinine</th>\n      <th>AST</th>\n      <th>ALT</th>\n      <th>Gtp</th>\n      <th>dental caries</th>\n      <th>gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55</td>\n      <td>165</td>\n      <td>60</td>\n      <td>81.0</td>\n      <td>0.5</td>\n      <td>0.6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>135</td>\n      <td>87</td>\n      <td>94</td>\n      <td>172</td>\n      <td>300</td>\n      <td>40</td>\n      <td>75</td>\n      <td>16.5</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>22</td>\n      <td>25</td>\n      <td>27</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>70</td>\n      <td>165</td>\n      <td>65</td>\n      <td>89.0</td>\n      <td>0.6</td>\n      <td>0.7</td>\n      <td>2</td>\n      <td>2</td>\n      <td>146</td>\n      <td>83</td>\n      <td>147</td>\n      <td>194</td>\n      <td>55</td>\n      <td>57</td>\n      <td>126</td>\n      <td>16.2</td>\n      <td>1</td>\n      <td>1.1</td>\n      <td>27</td>\n      <td>23</td>\n      <td>37</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>170</td>\n      <td>75</td>\n      <td>81.0</td>\n      <td>0.4</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>118</td>\n      <td>75</td>\n      <td>79</td>\n      <td>178</td>\n      <td>197</td>\n      <td>45</td>\n      <td>93</td>\n      <td>17.4</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>27</td>\n      <td>31</td>\n      <td>53</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35</td>\n      <td>180</td>\n      <td>95</td>\n      <td>105.0</td>\n      <td>1.5</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>131</td>\n      <td>88</td>\n      <td>91</td>\n      <td>180</td>\n      <td>203</td>\n      <td>38</td>\n      <td>102</td>\n      <td>15.9</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>20</td>\n      <td>27</td>\n      <td>30</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30</td>\n      <td>165</td>\n      <td>60</td>\n      <td>80.5</td>\n      <td>1.5</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>121</td>\n      <td>76</td>\n      <td>91</td>\n      <td>155</td>\n      <td>87</td>\n      <td>44</td>\n      <td>93</td>\n      <td>15.4</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>19</td>\n      <td>13</td>\n      <td>17</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>50</td>\n      <td>170</td>\n      <td>55</td>\n      <td>51.0</td>\n      <td>1.2</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>146</td>\n      <td>95</td>\n      <td>101</td>\n      <td>199</td>\n      <td>343</td>\n      <td>31</td>\n      <td>99</td>\n      <td>15.9</td>\n      <td>1</td>\n      <td>0.7</td>\n      <td>24</td>\n      <td>42</td>\n      <td>119</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>45</td>\n      <td>160</td>\n      <td>55</td>\n      <td>69.0</td>\n      <td>1.5</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>150</td>\n      <td>88</td>\n      <td>84</td>\n      <td>222</td>\n      <td>153</td>\n      <td>69</td>\n      <td>122</td>\n      <td>13.0</td>\n      <td>1</td>\n      <td>0.7</td>\n      <td>17</td>\n      <td>12</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>55</td>\n      <td>155</td>\n      <td>60</td>\n      <td>84.5</td>\n      <td>0.7</td>\n      <td>0.9</td>\n      <td>1</td>\n      <td>1</td>\n      <td>137</td>\n      <td>91</td>\n      <td>100</td>\n      <td>282</td>\n      <td>165</td>\n      <td>51</td>\n      <td>198</td>\n      <td>14.5</td>\n      <td>1</td>\n      <td>0.7</td>\n      <td>16</td>\n      <td>15</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>40</td>\n      <td>165</td>\n      <td>70</td>\n      <td>89.0</td>\n      <td>0.7</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>130</td>\n      <td>80</td>\n      <td>104</td>\n      <td>243</td>\n      <td>163</td>\n      <td>59</td>\n      <td>150</td>\n      <td>15.7</td>\n      <td>1</td>\n      <td>0.9</td>\n      <td>24</td>\n      <td>21</td>\n      <td>31</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>40</td>\n      <td>155</td>\n      <td>50</td>\n      <td>73.0</td>\n      <td>1.5</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>105</td>\n      <td>70</td>\n      <td>64</td>\n      <td>183</td>\n      <td>27</td>\n      <td>55</td>\n      <td>122</td>\n      <td>13.2</td>\n      <td>1</td>\n      <td>0.7</td>\n      <td>22</td>\n      <td>16</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[features].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:14.690174882Z",
     "start_time": "2023-11-07T21:41:14.675877622Z"
    }
   },
   "outputs": [],
   "source": [
    "# train[train['eyesight(left)'] > 9.0] = 0\n",
    "# train[train['eyesight(right)'] > 9.0] = 0\n",
    "\n",
    "# test[test['eyesight(left)'] > 9.0] = 0\n",
    "# test[test['eyesight(right)'] > 9.0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:17.900125318Z",
     "start_time": "2023-11-07T21:41:17.860971776Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "mm_scaler = MinMaxScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "# swap out which scaler you want here (some people scale inside of CV)\n",
    "train[features] = pd.DataFrame(mm_scaler.fit_transform(train[features]), columns = features)\n",
    "test[features] = pd.DataFrame(mm_scaler.transform(test[features]), columns = features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:18.577903058Z",
     "start_time": "2023-11-07T21:41:18.572520372Z"
    }
   },
   "outputs": [],
   "source": [
    "# define y variable\n",
    "y = train['smoking']\n",
    "\n",
    "x_test = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:19.816570193Z",
     "start_time": "2023-11-07T21:41:19.798590592Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "folds = 5\n",
    "seed = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:21.113388886Z",
     "start_time": "2023-11-07T21:41:21.106582104Z"
    }
   },
   "outputs": [],
   "source": [
    "# if continuous, change predict_proba to predict\n",
    "def run_cv_and_predict(train, test, features, model, seed,verbose=False):\n",
    "    \n",
    "    # initialize arrays \n",
    "    fold_scores = []\n",
    "    oof = np.zeros(train.shape[0])\n",
    "    preds = np.zeros((test.shape[0],folds))\n",
    "\n",
    "    # setup folding strategy\n",
    "    skf = StratifiedKFold(n_splits=folds,random_state = seed,shuffle=True)\n",
    "    # Use KFold if target is continuous\n",
    "    #skf = KFold(n_splits=folds,random_state = seed,shuffle=True)\n",
    "\n",
    "    # start cross validation\n",
    "    cur_fold = 1\n",
    "    for trn_idx, val_idx in skf.split(train[features], y):\n",
    "\n",
    "        # split indicies into train and validation\n",
    "        x_train = train[features].iloc[trn_idx]\n",
    "        y_train = y.iloc[trn_idx]\n",
    "        x_valid = train[features].iloc[val_idx]\n",
    "        y_valid = y.iloc[val_idx]\n",
    "\n",
    "        # fit model\n",
    "        model.fit(x_train,y_train)\n",
    "\n",
    "        # predict on validation set\n",
    "        fold_preds = model.predict_proba(x_valid)[:,1]\n",
    "        fold_preds = fold_preds.reshape(len(fold_preds)) # sbabwtdt\n",
    "        oof[val_idx] = fold_preds\n",
    "\n",
    "        # Compute scores\n",
    "        fold_score = roc_auc_score(y_valid,fold_preds)  # CHOOSE METRIC HERE\n",
    "        fold_scores.append(fold_score)\n",
    "        if verbose:\n",
    "            print(f'ROC AUC Score, fold {cur_fold}: {fold_score}')\n",
    "\n",
    "        # predict on test set - store all fold preds (take mode later)\n",
    "        test_preds = model.predict_proba(test[features])[:,1]\n",
    "        test_preds = test_preds.reshape(len(test_preds))  # shouldn't have to do this.\n",
    "        preds[:,cur_fold-1] = test_preds  \n",
    "        cur_fold +=1\n",
    "    \n",
    "    # Print mean fold and oof score \n",
    "    oof_score = roc_auc_score(y,oof)\n",
    "    # oof_score = np.sqrt(mean_squared_error(y,oof))\n",
    "    print(f'ROC AUC score: {np.mean(fold_scores):.5f}, Stdev: {np.std(fold_scores):.5f}, OOF score: {oof_score:.5f}')\n",
    "    # print(f'RMSE score: {np.mean(scores):.5f}, Stdev: {np.std(scores):.5f}, OOF score: {oof_score:.5f}')\n",
    "\n",
    "    return (preds,fold_scores,oof_score,oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:43.252000313Z",
     "start_time": "2023-11-07T21:41:43.193564384Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression   # try CV too\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:41:56.660926550Z",
     "start_time": "2023-11-07T21:41:53.354716596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.84196, Stdev: 0.00184, OOF score: 0.84196\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression()#C=10)\n",
    "preds_lr,scores_lr,oof_score_lr,oof_lr = run_cv_and_predict(train,test,features,model_lr,seed)\n",
    "\n",
    "\n",
    "# model.fit(x_train,y_train)\n",
    "# val_preds = model.predict_proba(x_valid)[:,1]  \n",
    "# score = roc_auc_score(y_valid, val_preds)   # make sure true is first\n",
    "# print(f'ROC AUC Score = {score:.5f}')\n",
    "\n",
    "\n",
    "# default lr, no feature engineering, all features, tts = .81622, lb = .81916\n",
    "# int features = .80181, lb = .80703\n",
    "# float features = .76748, lb = .77048\n",
    "# cross validation: .81702, lb = \n",
    "# std scaling helped logreg by alot! = .83371\n",
    "# add gender feature: .84207\n",
    "# better sex preds: .84197, lb = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5,3.5))\n",
    "# fpr,tpr,_ = roc_curve(y,oof_lr)\n",
    "# plt.plot(fpr,tpr, label=f'AUC = {np.mean(scores_lr):.5}')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('ROC AUC Score for Logistic Regression')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "* Slowest = about 4 min run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model_rf = RandomForestClassifier()\n",
    "\n",
    "# preds_rf,scores_rf,oof_score_rf,oof_rf = run_cv_and_predict(train,test,features,model_rf,seed)\n",
    "\n",
    "# RF default ROC AUC score: 0.85248, Stdev: 0.00173, OOF score: 0.85247, lb = \n",
    "# 4 min run time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost\n",
    "* About 2x speed of RF with current config, 2 min run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-07T21:42:06.343472977Z",
     "start_time": "2023-11-07T21:42:06.311966870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 ms, sys: 3.4 ms, total: 14.2 ms\n",
      "Wall time: 31.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "# results = []\n",
    "# for iters in [500,750,1000,1250,1500]:\n",
    "#     for d in [3,5,6,7,9]:\n",
    "#         for lr in [.001,.005,.01,.025,.05,.1]:\n",
    "#             model_cat = CatBoostClassifier(verbose=0, \n",
    "#                                            iterations=iters,\n",
    "#                                            depth=d,\n",
    "#                                            learning_rate=lr,\n",
    "#                                            task_type='GPU')\n",
    "#             print(f'iters: {iters}, d: {d}, lr: {lr}')\n",
    "#             preds_cat, scores_cat, oof_score_cat, oof_cat = run_cv_and_predict(train,test,features,model_cat,seed,verbose=False)\n",
    "#             results.append([iters,d,lr, np.mean(scores_cat),oof_score_cat])\n",
    "\n",
    "# default ROC AUC score: 0.86531, Stdev: 0.00177, OOF score: 0.86529, lb = \n",
    "# 2 min run time\n",
    "\n",
    "# add gender feature, 1000,.025,6, score = .86874, lb = .87257\n",
    "\n",
    "# best optimization model: 1500,5,0.1, cv = 0.86845, oof = 0.86844  ( a bit lower than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(results,columns=['iterations','depth','learning_rate','cv_score','oof_score'])\n",
    "# results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_row = results_df[results_df.cv_score == max(results_df.cv_score)]\n",
    "# print(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "\n",
    "# # plotly 3d surface\n",
    "# fig = px.scatter_3d(results_df, x='iterations', y='depth', z='learning_rate',color='cv_score',\n",
    "#                    title='Score vs hyperparams')  # scatter of 3 cols of df\n",
    "# #fig.update_traces(marker={'size': 4})  # scene=dict(zaxis=dict(range=[-5, 5]))\n",
    "# fig.show(renderer='notebook')\n",
    "\n",
    "# plotly doesn't show up in gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use static matplotlib since plotly not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re, seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# sns.set_style('whitegrid')\n",
    "\n",
    "# # axes instance\n",
    "# fig = plt.figure(figsize=(6,6))\n",
    "# ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "# fig.add_axes(ax)\n",
    "\n",
    "# # get colormap from seaborn\n",
    "# cmap = ListedColormap(sns.color_palette(\"viridis\", 256).as_hex())\n",
    "\n",
    "# # plot\n",
    "# sc = ax.scatter(results_df.iterations, results_df.depth, results_df.learning_rate, s=40, c=results_df.cv_score, marker='o', cmap=cmap, alpha=1)\n",
    "# ax.set_xlabel('Iterations')\n",
    "# ax.set_ylabel('Depth')\n",
    "# ax.set_zlabel('Learning Rate')\n",
    "\n",
    "# # legend\n",
    "# plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n",
    "\n",
    "# # save\n",
    "# plt.savefig(\"scatter_hue\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-08T17:23:54.204882949Z",
     "start_time": "2023-11-08T17:23:54.145388157Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "#     param_grid = {\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 5e-3, 2e-1),\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 5000,10000),\n",
    "#         'depth': trial.suggest_categorical('depth', [3,5,7,9,11]),\n",
    "#         #\"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "#         #\"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "#     }\n",
    "    \n",
    "    \n",
    "    # these are from catboost github\n",
    "    cat_param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 5e-3, 2e-1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 2000,10000),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 200),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 15),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"eval_metric\": \"Accuracy\",\n",
    "    }\n",
    "\n",
    "    if cat_param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        cat_param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif cat_param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        cat_param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n",
    "    \n",
    "    \n",
    "    #model = XGBClassifier(**param_grid,tree_method=\"gpu_hist\") #tree_method='hist',device='cuda')\n",
    "    model = CatBoostClassifier(**cat_param,\n",
    "                                verbose=0,\n",
    "                               task_type='GPU')\n",
    "    \n",
    "    # do cross validation - 5 folds\n",
    "    skf = StratifiedKFold(n_splits=folds,random_state = seed,shuffle=True)\n",
    "\n",
    "    # start cross validation\n",
    "    cur_fold = 1\n",
    "    for trn_idx, val_idx in skf.split(train[features], y):\n",
    "\n",
    "        # split indicies into train and validation\n",
    "        x_train = train[features].iloc[trn_idx]\n",
    "        y_train = y.iloc[trn_idx]\n",
    "        x_valid = train[features].iloc[val_idx]\n",
    "        y_valid = y.iloc[val_idx]\n",
    "\n",
    "        # fit model\n",
    "        model.fit(x_train,y_train)\n",
    "\n",
    "        # predict on validation set\n",
    "        fold_preds = model.predict_proba(x_valid)[:,1]\n",
    "        fold_score = roc_auc_score(y_valid,fold_preds) \n",
    "        scores.append(fold_score)\n",
    "        #print(f'ROC AUC Score, fold {cur_fold}: {fold_score}')\n",
    "\n",
    "        # predict on test set - store all fold preds (take mode later)\n",
    "#         test_preds = model.predict_proba(test[features])[:,1]\n",
    "#         test_preds = test_preds.reshape(len(test_preds))  # shouldn't have to do this.\n",
    "#         preds[:,cur_fold-1] = test_preds \n",
    "        \n",
    "        cur_fold +=1\n",
    "        \n",
    "    print(np.mean(scores))\n",
    "\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-08T18:42:37.944138035Z",
     "start_time": "2023-11-08T17:23:56.694350024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8691627598700936\n",
      "0.8586799545003185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-08 11:42:37,899] Trial 2 failed with parameters: {'learning_rate': 0.1758958846977275, 'n_estimators': 9157, 'objective': 'Logloss', 'min_data_in_leaf': 34, 'depth': 14, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 3.1551563100606295} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jon/mambaforge/envs/dataspell/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_184281/4236473728.py\", line 54, in objective\n",
      "    model.fit(x_train,y_train)\n",
      "  File \"/home/jon/mambaforge/envs/dataspell/lib/python3.8/site-packages/catboost/core.py\", line 5100, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"/home/jon/mambaforge/envs/dataspell/lib/python3.8/site-packages/catboost/core.py\", line 2319, in _fit\n",
      "    self._train(\n",
      "  File \"/home/jon/mambaforge/envs/dataspell/lib/python3.8/site-packages/catboost/core.py\", line 1723, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4645, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4694, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-08 11:42:37,899] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:6\u001B[0m\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/optuna/study/study.py:451\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    350\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    357\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    358\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \n\u001B[1;32m    361\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 451\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 66\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     79\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 163\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    244\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    247\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    250\u001B[0m ):\n\u001B[0;32m--> 251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 200\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    202\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    203\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[0;32mIn[79], line 54\u001B[0m, in \u001B[0;36mobjective\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m     51\u001B[0m y_valid \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39miloc[val_idx]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# fit model\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# predict on validation set\u001B[39;00m\n\u001B[1;32m     57\u001B[0m fold_preds \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict_proba(x_valid)[:,\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/catboost/core.py:5100\u001B[0m, in \u001B[0;36mCatBoostClassifier.fit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   5097\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_function\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[1;32m   5098\u001B[0m     CatBoostClassifier\u001B[38;5;241m.\u001B[39m_check_is_compatible_loss(params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_function\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m-> 5100\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcat_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbaseline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_best_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5101\u001B[0m \u001B[43m          \u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogging_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_description\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_period\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5102\u001B[0m \u001B[43m          \u001B[49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_snapshot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_interval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_cout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_cerr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/catboost/core.py:2319\u001B[0m, in \u001B[0;36mCatBoost._fit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   2315\u001B[0m allow_clear_pool \u001B[38;5;241m=\u001B[39m train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_clear_pool\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   2317\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m log_fixup(log_cout, log_cerr), \\\n\u001B[1;32m   2318\u001B[0m     plot_wrapper(plot, plot_file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining plots\u001B[39m\u001B[38;5;124m'\u001B[39m, [_get_train_dir(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_params())]):\n\u001B[0;32m-> 2319\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2320\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_pool\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2321\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meval_sets\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2322\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2323\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_clear_pool\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2324\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minit_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m   2325\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2327\u001B[0m \u001B[38;5;66;03m# Have property feature_importance possibly set\u001B[39;00m\n\u001B[1;32m   2328\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_object\u001B[38;5;241m.\u001B[39m_get_loss_function_name()\n",
      "File \u001B[0;32m~/mambaforge/envs/dataspell/lib/python3.8/site-packages/catboost/core.py:1723\u001B[0m, in \u001B[0;36m_CatBoostBase._train\u001B[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001B[0m\n\u001B[1;32m   1722\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_train\u001B[39m(\u001B[38;5;28mself\u001B[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001B[0;32m-> 1723\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_object\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_clear_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_object\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   1724\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_trained_model_attributes()\n",
      "File \u001B[0;32m_catboost.pyx:4645\u001B[0m, in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_catboost.pyx:4694\u001B[0m, in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create the study\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "sampler = TPESampler(seed=1)\n",
    "study = optuna.create_study(study_name=\"catboost\", direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-08T18:42:37.947519921Z",
     "start_time": "2023-11-08T18:42:37.944902812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  3\n",
      "Best trial:\n",
      "  Value:  0.8691627598700936\n",
      "  Params: \n",
      "    learning_rate: 0.08631929091700194\n",
      "    n_estimators: 7763\n",
      "    objective: CrossEntropy\n",
      "    min_data_in_leaf: 30\n",
      "    depth: 4\n",
      "    boosting_type: Plain\n",
      "    bootstrap_type: Bernoulli\n",
      "    subsample: 0.48441713902989497\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trial\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Value: \", best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 11/4/23 Trial Results:\n",
    "<pre>Number of finished trials:  20\n",
    "Best trial:\n",
    "  Value:  0.8683187056949443\n",
    "  Params: \n",
    "    learning_rate: 0.11287917716049423\n",
    "    n_estimators: 1528\n",
    "    depth: 5\n",
    "    min_data_in_leaf: 20\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/5/23 Results\n",
    "<pre>\n",
    "Number of finished trials:  20\n",
    "Best trial:\n",
    "  Value:  0.8687640116756683\n",
    "  Params: \n",
    "    learning_rate: 0.061979058943487576\n",
    "    n_estimators: 2099\n",
    "    depth: 5\n",
    "    min_data_in_leaf: 56\n",
    "</pre>\n",
    "\n",
    "<pre>\n",
    "Number of finished trials:  100\n",
    "Best trial:\n",
    "  Value:  0.8692235364730058\n",
    "  Params: \n",
    "    learning_rate: 0.06588173117955229\n",
    "    n_estimators: 2881\n",
    "    depth: 5\n",
    "    min_data_in_leaf: 33\n",
    "    \n",
    "</pre>\n",
    "\n",
    "## 11/6/23 Results\n",
    "<pre>\n",
    "Number of finished trials:  100\n",
    "Best trial:\n",
    "  Value:  0.8697138329104549\n",
    "  Params: \n",
    "    learning_rate: 0.0452122794667278\n",
    "    n_estimators: 4927\n",
    "    depth: 5\n",
    "    min_data_in_leaf: 100\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "# model_cat = pickle.load(open('/kaggle/input/s3e24-saved-models/catboost_optuna2.pkl', 'rb'))\n",
    "# preds_cat = model_cat.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set1_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# # set 1\n",
    "# model_xgb = XGBClassifier(#lambda=1.9550993816120785,\n",
    "#                           alpha= 5.814007913805305,\n",
    "#                           colsample_bytree= 0.3,\n",
    "#                           subsample= 1.0,\n",
    "#                           learning_rate= 0.0583947767053339,\n",
    "#                           n_estimators= 1070,\n",
    "#                           max_depth= 7,\n",
    "#                           min_child_weight= 64,\n",
    "#                           random_seed=seed,\n",
    "#                          )\n",
    "\n",
    "\n",
    "# set 2\n",
    "model_xgb = XGBClassifier(#lambda=1.9550993816120785,\n",
    "                      alpha= 4.632016934249366,\n",
    "                      colsample_bytree= 0.3,\n",
    "                      subsample= 1.0,\n",
    "                      learning_rate= 0.06313783399677664,\n",
    "                      n_estimators= 1136,\n",
    "                      max_depth= 7,\n",
    "                      min_child_weight= 53,\n",
    "                      random_state=seed\n",
    "                     )\n",
    "\n",
    "# # set 3\n",
    "# model_xgb = XGBClassifier(#lambda=1.9550993816120785,\n",
    "#                       alpha= 7.061,\n",
    "#                       colsample_bytree= 0.3,\n",
    "#                       subsample= 0.8,\n",
    "#                       learning_rate= 0.03416,\n",
    "#                       n_estimators= 1874,\n",
    "#                       max_depth= 7,\n",
    "#                       min_child_weight= 37,\n",
    "#                       random_state=seed\n",
    "#                      )\n",
    "\n",
    "\n",
    "# model_xgb = XGBClassifier(max_depth=4, n_estimators=100, learning_rate=.35)\n",
    "\n",
    "# print(f'learning rate: {lr}')\n",
    "\n",
    "#preds_xgb, scores_xgb, oof_score_xgb, oof_xgb = run_cv_and_predict(train,test,features,model_xgb,seed,verbose=True)\n",
    "\n",
    "# default, ROC AUC score: 0.86121, Stdev: 0.00183, OOF score: 0.86118, lb = .86831\n",
    "# tuning, higher estimators  = decreasing results.  Try just 50, currently 100 is the highest\n",
    "# max depth of 4 is .86147 (highest of 3-9)\n",
    "# lr .1 to .5, highest is .35 at .86160\n",
    "\n",
    "\n",
    "# Optuna run 1: ROC AUC score: 0.86783, Stdev: 0.00167, OOF score: 0.86782\n",
    "# eyesight >9.0 = 0.0, ROC AUC score: 0.86798, Stdev: 0.00245, OOF score: 0.86796\n",
    "\n",
    "# eyesight thing off, std scaler, ROC AUC score: 0.86783, Stdev: 0.00167, OOF score: 0.86782, lb = .87136\n",
    "# eyesight off, mm scaler: tad worse\n",
    "\n",
    "# add gender feature, no eye correction\n",
    "# set 1 hp's: cv = .87247, lb = .87587\n",
    "# set 2 hp's: cv = .87280, lb = .87623\n",
    "# set 3 hp's: cv = .87304, lb = .87610\n",
    "\n",
    "# use better model for sex\n",
    "# set 1 hp's: cv = 0.87255\n",
    "# set 2 hp's: cv = 0.87269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Full model with different random seeds and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# results = []\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "    \n",
    "# define the model\n",
    "# model_xgb = XGBClassifier(#lambda=1.9550993816120785,\n",
    "#                       alpha= 4.632016934249366,\n",
    "#                       colsample_bytree= 0.3,\n",
    "#                       subsample= 1.0,\n",
    "#                       learning_rate= 0.06313783399677664,\n",
    "#                       n_estimators= 1136,\n",
    "#                       max_depth= 7,\n",
    "#                       min_child_weight= 53,\n",
    "#                       random_state=i\n",
    "#                      )\n",
    "\n",
    "\n",
    "    \n",
    "#     # fit the model on the whole dataset\n",
    "#     model_xgb.fit(train[features],y)\n",
    "#     # make predictions\n",
    "#     results.append(model_xgb.predict_proba(x_test)[:,1])\n",
    "    \n",
    "# preds_xgb_full = np.mean(results,axis=0)\n",
    "\n",
    "    \n",
    "# 11/02/23 New params, score = .86798\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "* super fast ~= 40 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y lightgbm\n",
    "# !apt-get install -y libboost-all-dev\n",
    "# !git clone --recursive https://github.com/Microsoft/LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd LightGBM\n",
    "# rm -r build\n",
    "# mkdir build\n",
    "# cd build\n",
    "# cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\n",
    "# make -j$(nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd LightGBM/python-package/;python setup.py install --precompile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n",
    "# !rm -r LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T22:11:37.382070699Z",
     "start_time": "2023-11-07T22:11:25.581793873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 55722, number of negative: 71682\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2130\n",
      "[LightGBM] [Info] Number of data points in the train set: 127404, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.437365 -> initscore=-0.251865\n",
      "[LightGBM] [Info] Start training from score -0.251865\n",
      "ROC AUC Score, fold 1: 0.8721472326495876\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 55722, number of negative: 71683\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2140\n",
      "[LightGBM] [Info] Number of data points in the train set: 127405, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.437361 -> initscore=-0.251879\n",
      "[LightGBM] [Info] Start training from score -0.251879\n",
      "ROC AUC Score, fold 2: 0.8713806230580309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 55722, number of negative: 71683\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2132\n",
      "[LightGBM] [Info] Number of data points in the train set: 127405, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.437361 -> initscore=-0.251879\n",
      "[LightGBM] [Info] Start training from score -0.251879\n",
      "ROC AUC Score, fold 3: 0.8735012228637058\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 55723, number of negative: 71682\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2135\n",
      "[LightGBM] [Info] Number of data points in the train set: 127405, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.437369 -> initscore=-0.251847\n",
      "[LightGBM] [Info] Start training from score -0.251847\n",
      "ROC AUC Score, fold 4: 0.8698943112094467\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 55723, number of negative: 71682\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2132\n",
      "[LightGBM] [Info] Number of data points in the train set: 127405, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.437369 -> initscore=-0.251847\n",
      "[LightGBM] [Info] Start training from score -0.251847\n",
      "ROC AUC Score, fold 5: 0.8697601217243118\n",
      "ROC AUC score: 0.87134, Stdev: 0.00141, OOF score: 0.87132\n",
      "CPU times: user 2min 52s, sys: 255 ms, total: 2min 53s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# best params so far:\n",
    "est = 1000\n",
    "lr = .02\n",
    "d = 3\n",
    "\n",
    "# results = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lgbm_params = {'n_estimators': 624,\n",
    "               'max_depth': 46,\n",
    "               'learning_rate': 0.06953273561619135,\n",
    "               'min_child_weight': 2.4187716216112944,\n",
    "               'min_child_samples': 230,\n",
    "               'subsample': 0.9515130309407626,\n",
    "               'subsample_freq': 4,\n",
    "               'colsample_bytree': 0.402284262124352,\n",
    "               'num_leaves': 71}\n",
    "\n",
    "\n",
    "model_lgbm=LGBMClassifier(**lgbm_params)\n",
    "\n",
    "# # for i in range(10):\n",
    "# model_lgbm = LGBMClassifier(verbose=0,\n",
    "#                             n_estimators=est,\n",
    "#                             learning_rate=lr,\n",
    "#                             max_depth=d,\n",
    "#                             random_state=seed,\n",
    "#                             )\n",
    "\n",
    "#     # now that tuned fit the model on the whole dataset\n",
    "#     model_lgbm.fit(train[features],y)\n",
    "    \n",
    "#     # make predictions\n",
    "#     results.append(model_lgbm.predict_proba(x_test)[:,1])\n",
    "    \n",
    "# preds_lgbm_full = np.mean(results,axis=0)\n",
    "\n",
    "preds_lgbm, scores_lgbm, oof_score_lgbm, oof_lgbm = run_cv_and_predict(train,test,features,model_lgbm,seed,verbose=True)\n",
    "\n",
    "# default: ROC AUC score: 0.86130, Stdev: 0.00173, OOF score: 0.86128,\n",
    "# default 100 to 200 est: .86253\n",
    "# 200 to 400 est: .86307\n",
    "# 1000,.02,3, score = .86534\n",
    "\n",
    "# add gender feat, cv = .86353\n",
    "# 1000, .02,3: ROC AUC score: 0.86360, Stdev: 0.00150, OOF score: 0.86359\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LGBM Hyperparameter tuning - Optuna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T00:43:35.079412091Z",
     "start_time": "2023-11-08T00:43:35.032829454Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    lgbm_params = {'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "                   'max_depth': trial.suggest_int('max_depth', 20,100),\n",
    "                   'learning_rate': trial.suggest_float('learning_rate', 5e-3, 2e-1),\n",
    "                   'min_child_weight': trial.suggest_float('min_child_weight',1.0,5.0), \n",
    "                   'min_child_samples': trial.suggest_int('min_child_samples',100,400), \n",
    "                   'subsample': trial.suggest_float('subsample',0.3,1.0), \n",
    "                   'subsample_freq': trial.suggest_int('subsample_freq',3,20),\n",
    "                   'colsample_bytree': trial.suggest_float('colsample_bytree',0.2,1.00), \n",
    "                   'num_leaves': trial.suggest_int('num_leaves',5,120),\n",
    "                   }\n",
    "\n",
    "    model=LGBMClassifier(**lgbm_params,random_state=seed,verbose=-1)\n",
    "\n",
    "    # do cross validation - 5 folds\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "    # start cross validation\n",
    "    cur_fold = 1\n",
    "    for trn_idx, val_idx in skf.split(train[features], y):\n",
    "        # split indicies into train and validation\n",
    "        x_train = train[features].iloc[trn_idx]\n",
    "        y_train = y.iloc[trn_idx]\n",
    "        x_valid = train[features].iloc[val_idx]\n",
    "        y_valid = y.iloc[val_idx]\n",
    "\n",
    "        # fit model\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # predict on validation set\n",
    "        fold_preds = model.predict_proba(x_valid)[:, 1]\n",
    "        fold_score = roc_auc_score(y_valid, fold_preds)\n",
    "        scores.append(fold_score)\n",
    "        #print(f'ROC AUC Score, fold {cur_fold}: {fold_score}')\n",
    "\n",
    "        # predict on test set - store all fold preds (take mode later)\n",
    "        #         test_preds = model.predict_proba(test[features])[:,1]\n",
    "        #         test_preds = test_preds.reshape(len(test_preds))  # shouldn't have to do this.\n",
    "        #         preds[:,cur_fold-1] = test_preds \n",
    "\n",
    "        cur_fold += 1\n",
    "\n",
    "    print(np.mean(scores))\n",
    "\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8653542311531668\n",
      "0.8522548789500728\n",
      "0.8685433978684538\n",
      "0.8668262840418819\n",
      "0.8657491389171698\n",
      "0.8686826580318634\n",
      "0.8624623588268786\n",
      "0.869608133869097\n",
      "0.863687910008353\n",
      "0.8644760322911956\n",
      "0.8701434128615011\n",
      "0.8679979169418356\n",
      "0.8606910421896785\n",
      "0.8700656345692958\n",
      "0.8704008683166389\n",
      "0.8582178503606521\n",
      "0.8569725006825584\n",
      "0.86687018852195\n",
      "0.8575647191437911\n",
      "0.8699216659537296\n",
      "0.8664597422658311\n",
      "0.8702684810577754\n",
      "0.8698164270869235\n",
      "0.8630915933215431\n",
      "0.8613352566986794\n",
      "0.8663493920848016\n",
      "0.8615436679975472\n",
      "0.8643226015751289\n",
      "0.8595506446273221\n",
      "0.8613069157739943\n",
      "0.8634765444461197\n",
      "0.8701925233925026\n",
      "0.8694870629163294\n",
      "0.8681305074537814\n",
      "0.870607598478547\n",
      "0.8681640607301265\n",
      "0.8684751603937064\n",
      "0.8624926064131518\n",
      "0.8685050539029333\n",
      "0.8672210419284591\n",
      "0.8605023888777261\n",
      "0.8703190823759105\n",
      "0.8672020321967997\n",
      "0.86982067483414\n",
      "0.8702835529135431\n",
      "0.8659842381883521\n",
      "0.8696314957891577\n",
      "0.8641776183949998\n",
      "0.8699417102044004\n",
      "0.8692381543491896\n",
      "0.8651426961583691\n",
      "0.8703278417319963\n",
      "0.8695943305458321\n",
      "0.8703012600330327\n",
      "0.869988415460422\n",
      "0.8652668767125833\n",
      "0.8688711902067661\n",
      "0.8575996174458493\n",
      "0.8690930566513755\n",
      "0.8691529234044552\n",
      "0.8674208040639557\n",
      "0.870275567507959\n",
      "0.8700571956112999\n",
      "0.8702014745037076\n",
      "0.8680915440322681\n",
      "0.8668646704787761\n",
      "0.8600373717587054\n",
      "0.8696964800683922\n",
      "0.8691306998447617\n",
      "0.8701937170509855\n",
      "0.8673703101774715\n",
      "0.8702349763273306\n",
      "0.8680780105055852\n",
      "0.8694291991940025\n",
      "0.8688367451584474\n",
      "0.8688043218298949\n",
      "0.8701900570564824\n",
      "0.8681772515008965\n",
      "0.8645289233137877\n",
      "0.8602839576065044\n",
      "0.8657065473711301\n",
      "0.8700998020191596\n",
      "0.8702340715428019\n",
      "0.868515823039702\n",
      "0.8687807584768734\n",
      "0.8696704888482211\n",
      "0.8602628231403097\n",
      "0.8703305896305296\n",
      "0.8693486311828156\n",
      "0.8662692427600129\n",
      "0.869834328867061\n",
      "0.8703493382428503\n",
      "0.8691597196998775\n",
      "0.870467310698954\n",
      "0.8707892680944633\n",
      "0.8682162755754073\n",
      "0.864847906514958\n",
      "0.8692600983171029\n",
      "0.866927991780984\n",
      "0.8661556267573589\n",
      "0.87048573920477\n",
      "0.8708233734732065\n",
      "0.8688837890560894\n",
      "0.8700572772020445\n",
      "0.8707297823418759\n",
      "0.8684153442710378\n",
      "0.8703535139798507\n",
      "0.8579795452283305\n",
      "0.8690658015784896\n",
      "0.8670319959053323\n",
      "0.8696834020042974\n",
      "0.8707806355081413\n",
      "0.8693223868296511\n",
      "0.8705429999386178\n",
      "0.8685583450535599\n",
      "0.8701831767257246\n",
      "0.8707300842434271\n",
      "0.855630754492698\n",
      "0.8693057545958304\n",
      "0.8667766257957018\n",
      "0.8651983312779808\n",
      "0.8706726840752868\n",
      "0.870847135246081\n",
      "0.8694917591681749\n",
      "0.8700739649343422\n",
      "0.8689597779557143\n",
      "0.8707251088477796\n",
      "0.8704813367469846\n",
      "0.8706693211491526\n",
      "0.8675141696281191\n",
      "0.8684857704700978\n",
      "0.8705319014901134\n",
      "0.8708953435482281\n",
      "0.8694173052180295\n",
      "0.8561225737219734\n",
      "0.8702645394631953\n",
      "0.8695925628364078\n",
      "0.8658816487906833\n",
      "0.8703191639526231\n",
      "0.869817646540346\n",
      "0.869146966418851\n",
      "0.8703150587275882\n",
      "0.8706599619074957\n",
      "0.8551121230882724\n",
      "0.8700031671137503\n",
      "0.8692664066113117\n",
      "0.8705114959583108\n",
      "0.8692014671894726\n",
      "0.8700979993254583\n",
      "0.8665372998404941\n",
      "0.8707908955394152\n",
      "0.8706860836638727\n",
      "0.8704815214293564\n",
      "0.8696917854717932\n",
      "0.8706706192296665\n",
      "0.8700705340988701\n",
      "0.8616102144067217\n",
      "0.8681579788282308\n",
      "0.869345400708965\n",
      "0.8564685770429399\n",
      "0.8703894284560579\n",
      "0.8705826893501986\n",
      "0.8702691728090397\n",
      "0.8705545515815322\n",
      "0.8687544273590685\n",
      "0.8702336150672088\n",
      "0.8684548446990622\n",
      "0.8701933971138944\n",
      "0.870669007800565\n",
      "0.8674212566854564\n",
      "0.8669070924133593\n",
      "0.8707565108938062\n",
      "0.8700268153819841\n",
      "0.8707470261900241\n",
      "0.8704189880796805\n",
      "0.8689237996985455\n",
      "0.8705734775633214\n",
      "0.8701707052504496\n",
      "0.8698087543868709\n",
      "0.8705939632811756\n",
      "0.8698071360583928\n",
      "0.8708018317588756\n",
      "0.8706952203948088\n",
      "0.8708079193480689\n",
      "0.8707928154189697\n",
      "0.8708267574071421\n",
      "0.8706283601417448\n",
      "0.8695501352269026\n",
      "0.8679668984776774\n",
      "0.870658388451911\n",
      "0.869458371335767\n",
      "0.870649611974104\n",
      "0.871190761206422\n",
      "0.8710836352558955\n",
      "0.870818929733374\n",
      "0.8706982218182837\n",
      "0.8708060338901099\n",
      "0.8690475220476527\n",
      "0.8704448499440083\n",
      "0.8706267667251482\n",
      "0.8683452836079322\n",
      "0.8710664547059526\n",
      "0.8709653923368901\n",
      "0.8710210034125826\n",
      "0.8630382552517677\n",
      "0.8700686107528298\n",
      "0.87106474982675\n",
      "0.8705664276626761\n",
      "0.8703915626194337\n",
      "0.8697950247889583\n",
      "0.8711946181031838\n",
      "0.8710393955250761\n",
      "0.8710494645394113\n",
      "0.8713324140296084\n",
      "0.8703190245484702\n",
      "0.8711355076342837\n",
      "0.8709817219277991\n",
      "0.864543854651861\n",
      "0.8697913647493063\n",
      "0.8709829601701798\n",
      "0.8687407462627986\n",
      "0.8712378251697419\n",
      "0.8708339165257465\n",
      "0.8709026781340803\n",
      "0.8710808759396773\n",
      "0.8696734276857099\n",
      "0.8702797139464817\n",
      "0.8707022213791085\n",
      "0.8695784175783376\n",
      "0.8708254791051665\n",
      "0.8710256524335444\n",
      "0.8707123772240426\n",
      "0.8710723078766822\n",
      "0.8707152815699924\n",
      "0.8706762741211609\n",
      "0.8702446551182715\n",
      "0.8704307631548002\n",
      "0.8711510215012375\n",
      "0.8694691172493154\n",
      "0.8704920731007437\n",
      "0.8712092880444591\n",
      "0.8710714914153179\n",
      "0.871300185831245\n",
      "0.8712996475765966\n",
      "0.8714171347923514\n",
      "0.8714718607985983\n",
      "0.871355607699282\n",
      "0.8714841279999039\n",
      "0.8714197938796262\n",
      "0.8713157314414817\n",
      "0.8713133072137109\n",
      "0.8711883488715788\n",
      "0.871157523181709\n",
      "0.8710772491152973\n",
      "0.8706049016711314\n",
      "0.8712064761669899\n",
      "0.8712131121216334\n",
      "0.8715472231762795\n",
      "0.860898549542919\n",
      "0.8711558476521439\n",
      "0.871226642554843\n",
      "0.8710221219424458\n",
      "0.871428567546878\n",
      "0.8712619941512214\n",
      "0.8713982225540627\n",
      "0.8713820102638049\n",
      "0.8715781841895989\n",
      "0.8635399407607484\n",
      "0.8599876930774171\n",
      "0.8706199682293791\n",
      "0.8713598528010742\n",
      "0.8714766175550037\n",
      "0.8714633860304348\n",
      "0.8713689381417626\n",
      "0.8714667945492636\n",
      "0.8714374698653586\n",
      "0.8711847452437714\n",
      "0.8622623166784393\n",
      "0.8715481734171551\n",
      "0.8712561024701013\n",
      "0.8715209188535104\n",
      "0.8714362389941794\n",
      "0.8714026379699489\n",
      "0.8714106430113064\n",
      "0.8713587874648523\n",
      "0.8716630460240282\n",
      "0.871480415057202\n",
      "0.8714973591200144\n",
      "0.871437200017587\n",
      "0.8715095875498292\n",
      "0.8715982990895169\n",
      "0.8715028071010067\n",
      "0.8707818921930188\n",
      "0.8706959860160742\n",
      "0.8712143293047635\n",
      "0.8713745242012901\n",
      "0.8708044234647272\n",
      "0.8713454044593053\n",
      "0.8711744828792384\n",
      "0.8713630862892019\n",
      "0.8711624265863694\n",
      "0.8706762303299709\n",
      "0.8701849194312719\n",
      "0.871354109421436\n",
      "0.8710391890772435\n",
      "0.870918807836205\n",
      "0.8708394880833502\n",
      "0.8713608449547572\n",
      "0.8720357023843508\n",
      "0.8683355416738754\n",
      "0.8713490060424114\n",
      "0.8716765380801755\n",
      "0.8702650965989112\n",
      "0.8721359836170244\n",
      "0.8719668915923646\n",
      "0.8699020246561039\n",
      "0.8715877189747097\n",
      "0.8716083018248895\n",
      "0.8717599495452537\n",
      "0.8718010048562354\n",
      "0.8719846996599621\n",
      "0.8719022598707417\n",
      "0.8720119334537382\n",
      "0.8719570479650522\n",
      "0.8719206665591448\n",
      "0.8719168047316537\n",
      "0.8719751556022859\n",
      "0.8718904863686525\n",
      "0.8718728011567268\n",
      "0.8717401746970971\n",
      "0.8716949032934276\n",
      "0.8717832593730022\n",
      "0.8720032063633927\n",
      "0.8720056284801597\n",
      "0.8718058038810377\n",
      "0.8717132978869344\n",
      "0.8718311610308553\n",
      "0.8717933834515958\n",
      "0.8718420199384023\n",
      "0.8718501764951837\n",
      "0.8720003358623665\n",
      "0.8717980387418626\n",
      "0.8718987533387639\n",
      "0.8717121497743558\n",
      "0.8717483092365395\n",
      "0.8718900863253024\n",
      "0.8719481667359114\n",
      "0.8719699618905927\n",
      "0.8720066483387667\n",
      "0.8719183973691156\n",
      "0.8720299074756621\n",
      "0.8719794707049671\n",
      "0.8720736947901742\n",
      "0.871966961802553\n",
      "0.8720502146562892\n",
      "0.8720615439384474\n",
      "0.8721044359769319\n",
      "0.8721564039317802\n",
      "0.8720064659786999\n",
      "0.8717813733531067\n",
      "0.8718581312066325\n",
      "0.8721671713591445\n",
      "0.8719770958702029\n",
      "0.8711075627650355\n",
      "0.8716705943021072\n",
      "0.8720014694464266\n",
      "0.8719496443573685\n",
      "0.8721076547219037\n",
      "0.8713783514979377\n",
      "0.8719548141285574\n",
      "0.871104037336228\n",
      "0.8720026644693417\n",
      "0.8719028591743028\n",
      "0.8718723813635343\n",
      "0.871999411194925\n",
      "0.8712807691595383\n",
      "0.8710719314647152\n",
      "0.8720442868047098\n",
      "0.8719301130959248\n",
      "0.8719409277602939\n",
      "0.8721836231866371\n",
      "0.8714708203229172\n",
      "0.8722422076624012\n",
      "0.8720679430611291\n",
      "0.8714743924409337\n",
      "0.8720643426987925\n",
      "0.8713145909503366\n",
      "0.8718337734586742\n",
      "0.8723736064321331\n",
      "0.8720638324443243\n",
      "0.871995551558572\n",
      "0.8720247755619633\n",
      "0.8716766019318574\n",
      "0.8721694933626913\n",
      "0.871272015770115\n",
      "0.8721931870994954\n",
      "0.8722326691698887\n",
      "0.8719846104811886\n",
      "0.8708345893345466\n",
      "0.8719399116382636\n",
      "0.8719903288962415\n",
      "0.8713627266451164\n",
      "0.8721816272605736\n",
      "0.8717754204835313\n",
      "0.8720988965617119\n",
      "0.8715251623227065\n",
      "0.8720790706366106\n",
      "0.8720557225037181\n",
      "0.8714169985639094\n",
      "0.871335053325242\n",
      "0.8720168238531535\n",
      "0.8719547726345522\n",
      "0.8720024783898083\n",
      "0.8720892453442619\n",
      "0.8715469891618255\n",
      "0.8721062642048523\n",
      "0.8714679848053851\n",
      "0.8700235732363455\n",
      "0.8718138159054188\n",
      "0.8714835090586728\n",
      "0.8719627346342562\n",
      "0.8714350334989197\n",
      "0.8708355778496625\n",
      "0.8721101494740422\n",
      "0.8718984094014208\n",
      "0.8702822408049163\n",
      "0.8704774131484072\n",
      "0.8719523913720035\n",
      "0.872156519893084\n",
      "0.8714355489806025\n",
      "0.8719252110417793\n",
      "0.8721442163377517\n",
      "0.8722266716389727\n",
      "0.872281424132286\n",
      "0.8713943919891814\n",
      "0.8722631325596503\n",
      "0.8721837832414607\n",
      "0.8699528952951867\n",
      "0.8721891108436562\n",
      "0.8722073538883925\n",
      "0.8711589780586086\n",
      "0.8704963891325932\n",
      "0.8722959423017176\n",
      "0.8718017415956016\n",
      "0.8723066242249079\n",
      "0.8723448344052276\n",
      "0.8717737439377562\n",
      "0.8719297213885533\n",
      "0.8705935708000453\n",
      "0.8723768458146175\n",
      "0.8723032560129891\n",
      "0.871844447182491\n",
      "0.8724206722085659\n",
      "0.8719053171040763\n",
      "0.8719110036889731\n",
      "0.8723014567438575\n",
      "0.8724488494052076\n",
      "0.8718674801931836\n",
      "0.8724145024912809\n",
      "0.8709402560900378\n",
      "0.8715978897248258\n",
      "0.8722880432673821\n",
      "0.8718302980097123\n",
      "0.872710516783869\n",
      "0.8717291477348599\n",
      "0.8724451715879441\n",
      "0.871852144183397\n",
      "0.8719721577576902\n",
      "0.8707765938652591\n",
      "0.8723680729110537\n",
      "0.8714744341221847\n",
      "0.8719541949256971\n",
      "0.8708134064232752\n",
      "0.8723599307082438\n",
      "0.8724018757035511\n",
      "0.8714090329434951\n",
      "0.8722766651459027\n",
      "0.8715001161146052\n",
      "0.869966635336356\n",
      "0.8717084731242014\n",
      "0.8727878849234727\n",
      "0.8717974572620225\n",
      "0.8717722820050587\n",
      "0.8719444452974244\n",
      "0.8718521507146064\n",
      "0.8697148907454517\n",
      "0.8725090688571884\n",
      "0.8703298778312514\n",
      "0.8711867977344914\n",
      "0.871855754773945\n",
      "0.8721505723603293\n",
      "0.8722182527815775\n",
      "0.8707812975088753\n",
      "0.8717582717690971\n",
      "0.8712972353534367\n",
      "0.8713155901636462\n",
      "0.8711473713137521\n",
      "0.8712641627029768\n",
      "0.8719623473533374\n",
      "0.8687184146403151\n",
      "CPU times: user 17h 37min, sys: 56.3 s, total: 17h 37min 56s\n",
      "Wall time: 1h 8min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create the study\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "sampler = TPESampler(seed=1)\n",
    "study = optuna.create_study(study_name=\"lightgbm\", direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=500)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T01:52:03.770191047Z",
     "start_time": "2023-11-08T00:43:37.467526287Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  500\n",
      "Best trial:\n",
      "  Value:  0.8727878849234727\n",
      "  Params: \n",
      "    n_estimators: 1171\n",
      "    max_depth: 98\n",
      "    learning_rate: 0.053088573664819214\n",
      "    min_child_weight: 3.9905114400596657\n",
      "    min_child_samples: 388\n",
      "    subsample: 0.7482494463491081\n",
      "    subsample_freq: 14\n",
      "    colsample_bytree: 0.236449537072608\n",
      "    num_leaves: 36\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trial\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Value: \", best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T01:52:03.802347471Z",
     "start_time": "2023-11-08T01:52:03.786222290Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# import random\n",
    "\n",
    "# # model 1\n",
    "# model1 = oof_cat\n",
    "# preds_1 = preds_cat\n",
    "\n",
    "# # models 2\n",
    "# model2 = oof_xgb\n",
    "# preds_2 = preds_xgb\n",
    "\n",
    "# # model 3\n",
    "# model3 = oof_lgbm\n",
    "# preds_3 = preds_lgbm\n",
    "\n",
    "# # model 4\n",
    "# # model4 = oof_hgbc_ens\n",
    "# # preds_4 = hgbc_preds\n",
    "\n",
    "# # model 5\n",
    "# # model5 = oof_lr\n",
    "# # preds_5 = preds_lr\n",
    "\n",
    "# # generate a random mix for a stochastic determination of min\n",
    "# m = []\n",
    "# for i in range(1000):\n",
    "#     # Generate 3 random numbers to make a mix\n",
    "#     a,b,c = random.random(),random.random(),random.random() #, random.random(),random.random()\n",
    "#     total = a + b + c #+ d# + e\n",
    "#     a /= total\n",
    "#     b /= total\n",
    "#     c /= total\n",
    "# #     d /= total\n",
    "# #     e /= total\n",
    "    \n",
    "#     tm_ens = a*model1 + b*model2 + c*model3 # + d*model4# + e*model5\n",
    "#     tm_score = roc_auc_score(y,tm_ens)  # swap out error function here\n",
    "#     m.append([a,b,c, tm_score])\n",
    "    \n",
    "    \n",
    "# three_model = pd.DataFrame(m,columns=['a','b','c','score'])\n",
    "# three_model.head()\n",
    "\n",
    "# # find min/max score\n",
    "# max_row = three_model[three_model.score == three_model.score.max()]\n",
    "\n",
    "# # get ratios\n",
    "# ratio1 = max_row.a.values[0]\n",
    "# ratio2 = max_row.b.values[0]\n",
    "# ratio3 = max_row.c.values[0]\n",
    "# # ratio4 = max_row.d.values[0]\n",
    "# #ratio5 = max_row.e.values[0]\n",
    "\n",
    "# # predictions on test set\n",
    "# preds_ens = ratio1*preds_1 + \\\n",
    "#             ratio2*preds_2 + \\\n",
    "#             ratio3*preds_3 # + \\\n",
    "# #             ratio4*preds_4 + \\\n",
    "# #            ratio5+preds_5.mean(axis=1)\n",
    "\n",
    "\n",
    "# oof_3_model_ens = ratio1*oof_cat + ratio2*oof_xgb + ratio3*oof_lgbm # + ratio4*oof_hgbc_ens\n",
    "# score = roc_auc_score(y,oof_3_model_ens)\n",
    "# print(f'OOF score of ensemble: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plotly 3d surface\n",
    "# fig = px.scatter_3d(three_model, x='a', y='b', z='c',color='score',\n",
    "#                    title='Score vs ratios for 3 model ensemble')  # scatter of 3 cols of df\n",
    "# fig.update_traces(marker={'size': 4})  # scene=dict(zaxis=dict(range=[-5, 5]))\n",
    "# fig.show(renderer='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# voting_classifier = VotingClassifier(estimators=[\n",
    "#     ('catboost', model_cat),\n",
    "#     ('lgbm', model_lgbm),\n",
    "#     ('xgb', model_xgb)\n",
    "# ], voting='soft')\n",
    "\n",
    "# # voting_classifier.fit(train, y)\n",
    "# # y_pred = voting_classifier.predict(val_X)\n",
    "# # auroc_score = roc_auc_score(val_Y, y_pred)\n",
    "# # print(f\"AUC-ROC score on validation set: {auroc_score}\")\n",
    "\n",
    "# cv_scores = cross_val_score(voting_classifier, train[features], y, cv=5, scoring='roc_auc')\n",
    "\n",
    "# print(\"Cross-validation scores:\", cv_scores)\n",
    "# print(\"Mean AUC-ROC score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_classifier.fit(train[features], y)\n",
    "# voting_preds = voting_classifier.predict_proba(x_test[features])[:, 1]\n",
    "\n",
    "# ['smoking'] = test_preds.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Visualization Improvements\n",
    "* rotate plane to coincide with the z=0 plane, like PCA\n",
    "* stack planes with transparency to get another visualization dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort values so best scores are on the top\n",
    "# sorted_3m = three_model.sort_values(by='score',ascending=False)\n",
    "\n",
    "# # take the mean ratios of the top 10 results\n",
    "# a = sorted_3m.a[0:10].mean()\n",
    "# b = sorted_3m.b[0:10].mean()\n",
    "# c = sorted_3m.c[0:10].mean()\n",
    "# #d = sorted_3m.d[0:10].mean()\n",
    "\n",
    "# tot = a + b + c #+ d\n",
    "\n",
    "# # make a prediction using the average of the top 10 ratios\n",
    "# preds_ens = (a/tot)*preds_1.mean(axis=1) + (b/tot)*preds_2.mean(axis=1) + (c/tot)*preds_3.mean(axis=1) # + (d/tot)*preds_4  \n",
    "\n",
    "# oof_3_model_ens = (a/tot)*oof_cat + (b/tot)*oof_xgb + (c/tot)*oof_lgbm # + (d/tot)*oof_hgbc_ens\n",
    "# score = roc_auc_score(y,oof_3_model_ens)\n",
    "# print(f'OOF score of ensemble: {score:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chose Predictions and Submit Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = (preds_xgb_full + preds_lgbm_full)/2\n",
    "preds = preds_cat.mean(axis=1) # + preds_cat)/2\n",
    "#preds = voting_preds\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.smoking = preds\n",
    "ss.to_csv('231107_cat_submission.csv',index=False)\n",
    "ss.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3.5))\n",
    "ss.smoking.plot(kind='hist',bins=50)\n",
    "plt.title('Test Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
